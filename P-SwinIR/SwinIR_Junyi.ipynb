{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JbcPqAuI8cRAIEsJdi_d3JuNRJtljGsL","timestamp":1676356082368}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["# \n","try:\n","  from google.colab import drive\n","  drive.mount('/content/gdrive', force_remount=True)\n","\n","  FOLDERNAME = \"SWIN-IR_Junyi\"\n","  %cd /content/gdrive/MyDrive/$FOLDERNAME\n","except ImportError:\n","  pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3XxXNK1_lnW","executionInfo":{"status":"ok","timestamp":1676508842999,"user_tz":480,"elapsed":25596,"user":{"displayName":"JOEY (Joey)","userId":"00694724117728104285"}},"outputId":"2aac2c0a-92ac-46be-8c77-d080edd713e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/SWIN_IR_Junyi\n"]}]},{"cell_type":"code","source":["!pip install timm\n","!pip install h5py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bqkFQ2dY_uW","executionInfo":{"status":"ok","timestamp":1676508851064,"user_tz":480,"elapsed":8079,"user":{"displayName":"JOEY (Joey)","userId":"00694724117728104285"}},"outputId":"0d71b259-35ab-490e-b0ba-6bd1e27c736d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.1+cu116)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.1+cu116)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.25.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.9.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (4.0.0)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.12.0 timm-0.6.12\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.21.6)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUyO_Q6Fb9Q6"},"outputs":[],"source":["  import math\n","  import torch\n","  import torch.nn as nn\n","  import torch.nn.functional as F\n","  import torch.utils.checkpoint as checkpoint\n","  from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","\n","  from scipy.io.matlab.mio import savemat\n","  from scipy.io.matlab.mio import loadmat\n","  import torch.nn.init as init\n","  from math import log10\n","  import torch.optim as optim\n","  from torch.utils.data import DataLoader\n","  from torch.utils.data import Dataset\n","  import torchvision.transforms as transforms\n","  from PIL import Image, ImageFilter\n","  from os import listdir\n","  from os.path import join\n","  from scipy.io import loadmat\n","  from tqdm import tqdm\n","  import h5py"]},{"cell_type":"code","source":["# ########## Load Data #########\n","# # Here is the code how I genenrate datasets. \n","# # Please load the .mat data instead of the .h5. \n","# # The .h5 file will somehow cause the colab kernal crush. \n","\n"," \n","# f = h5py.File(\"nskt[256,uvw].h5\", mode=\"r\")\n","# w = f[\"w\"][()][::6]\n","# u = f[\"u\"][()][::6]\n","# v = f[\"v\"][()][::6]\n","# # in h5 file channel is for different sub-region\n","\n","# print(w.shape) # (1800//3=600,2,256,256)\n","# print(u.shape)\n","\n","# #channels stand for different sub-regions \n","\n","# # 80 % train 20 % test \n","# train_w = w[:400,0,...]\n","# train_u = u[:400,0,...]\n","# train_v = v[:400,0,...]\n","# train_w = train_w.reshape(train_w.shape[0],1,w.shape[2],w.shape[3])\n","# train_u = train_u.reshape(train_w.shape[0],1,w.shape[2],w.shape[3])\n","# train_v = train_v.reshape(train_w.shape[0],1,w.shape[2],w.shape[3])\n","\n","# test_w = w[::6,1,...]\n","# test_u = u[::6,1,...]\n","# test_v = v[::6,1,...]\n","# test_w = test_w.reshape(test_w.shape[0],1,w.shape[2],w.shape[3])\n","# test_u = test_u.reshape(test_w.shape[0],1,w.shape[2],w.shape[3])\n","# test_v = test_v.reshape(test_w.shape[0],1,w.shape[2],w.shape[3])\n","\n","# traindata = np.concatenate((train_w,train_u,train_v),axis =1)\n","# testdata = np.concatenate((test_w,test_u,test_v),axis =1)\n","# print(traindata.shape)\n","# print(testdata.shape)\n","# matdic = {'train': traindata,\n","#           'test': testdata}\n","# savemat('nskt_T400_C3_S256.mat',matdic)"],"metadata":{"id":"NDQZr5ci4BTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Parameters ############\n","\n","BATCH_SIZE = 4 \n","NUM_WORKERS = 0 # on Windows, set this variable to 0\n","scale_factor = 4\n","nb_epochs = 300 # typically kernel will crush after 500 epochs\n","cuda = True\n","CROP_SIZE = 256\n","\n","device = torch.device(\"cuda\" if (torch.cuda.is_available() and cuda) else \"cpu\")\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)"],"metadata":{"id":"Lr91v6E2_UdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########## Datasets ############\n","\n","\n","class DatasetFromTensor(Dataset):\n","    def __init__(self, data, scale_factor, with_bicubic_upsampling = True):\n","        super(DatasetFromTensor, self).__init__()\n","        self.data = data\n","\n","        crop_size = CROP_SIZE - (CROP_SIZE % scale_factor) # Valid crop size\n","        \n","        if with_bicubic_upsampling:\n","            self.input_transform = transforms.Compose([transforms.CenterCrop(crop_size), # cropping the image\n","                                        transforms.Resize(crop_size//scale_factor),  # subsampling the image (half size)\n","                                        transforms.Resize(crop_size, interpolation=Image.BICUBIC)  # bicubic upsampling to get back the original size \n","                                        ])\n","        else:\n","            self.input_transform = transforms.Compose([transforms.CenterCrop(crop_size), # cropping the image\n","                                        transforms.Resize(crop_size//scale_factor)  # subsampling the image (half size)\n","                                        ])\n","                \n","        self.target_transform = transforms.Compose([transforms.CenterCrop(crop_size) # since it's the target, we keep its original quality\n","                                        ])\n","\n","    def __getitem__(self, index):\n","        input = self.data[index]\n","        target = input.clone()\n","\n","        GB = transforms.GaussianBlur(kernel_size=(3,3), sigma=(1,1))\n","        \n","        input = GB(input)\n","        input = self.input_transform(input)\n","        target = self.target_transform(target)\n","\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.data)\n"],"metadata":{"id":"hQku9f--4-y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","data = loadmat('nskt_T400_C3_S256.mat')\n","trainValset = DatasetFromTensor(torch.tensor(data['train']), scale_factor=scale_factor, with_bicubic_upsampling=False)\n","testset = DatasetFromTensor(torch.tensor(data['test']), scale_factor=scale_factor, with_bicubic_upsampling=False)\n","trianset,valset = random_split(trainValset,[0.8,0.2],generator=torch.Generator().manual_seed(0))\n","trainloader = DataLoader(dataset=trianset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","valloader =  DataLoader(dataset=valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n","testloader = DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"],"metadata":{"id":"cu_HjpJf6A3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########## Model #############\n","\n","\n","class Mlp(nn.Module):  #MLP\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class WindowAttention(nn.Module):  #MSA\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n","\n","    def flops(self, N):\n","        # calculate flops for 1 window with token length of N\n","        flops = 0\n","        # qkv = self.qkv(x)\n","        flops += N * self.dim * 3 * self.dim\n","        # attn = (q @ k.transpose(-2, -1))\n","        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n","        #  x = (attn @ v)\n","        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n","        # x = self.proj(x)\n","        flops += N * self.dim * self.dim\n","        return flops\n","\n","\n","class SwinTransformerBlock(nn.Module):  #STL\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        if min(self.input_resolution) <= self.window_size:\n","            # if window size is larger than input resolution, we don't partition windows\n","            self.shift_size = 0\n","            self.window_size = min(self.input_resolution)\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if self.shift_size > 0:\n","            attn_mask = self.calculate_mask(self.input_resolution)\n","        else:\n","            attn_mask = None\n","\n","        self.register_buffer(\"attn_mask\", attn_mask)\n","\n","    def calculate_mask(self, x_size):\n","        # calculate attention mask for SW-MSA\n","        H, W = x_size\n","        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","        return attn_mask\n","\n","    def forward(self, x, x_size):\n","        H, W = x_size\n","        B, L, C = x.shape\n","        # assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","\n","        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n","        if self.input_resolution == x_size:\n","            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n","        else:\n","            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n","               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.input_resolution\n","        # norm1\n","        flops += self.dim * H * W\n","        # W-MSA/SW-MSA\n","        nW = H * W / self.window_size / self.window_size\n","        flops += nW * self.attn.flops(self.window_size * self.window_size)\n","        # mlp\n","        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n","        # norm2\n","        flops += self.dim * H * W\n","        return flops\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.dim\n","        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n","        return flops\n","\n","\n","class BasicLayer(nn.Module):    #STLs (series of STL inside RSTB)\n","  \"\"\" A basic Swin Transformer layer for one stage.\n","\n","  Args:\n","      dim (int): Number of input channels.\n","      input_resolution (tuple[int]): Input resolution.\n","      depth (int): Number of blocks.\n","      num_heads (int): Number of attention heads.\n","      window_size (int): Local window size.\n","      mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","      qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","      qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","      drop (float, optional): Dropout rate. Default: 0.0\n","      attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","      drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","      norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","      downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","      use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","  \"\"\"\n","\n","  def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","\n","      super().__init__()\n","      self.dim = dim\n","      self.input_resolution = input_resolution\n","      self.depth = depth\n","      self.use_checkpoint = use_checkpoint\n","\n","      # build blocks\n","      self.blocks = nn.ModuleList([\n","          SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                num_heads=num_heads, window_size=window_size,\n","                                shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                mlp_ratio=mlp_ratio,\n","                                qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                drop=drop, attn_drop=attn_drop,\n","                                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                norm_layer=norm_layer)\n","          for i in range(depth)])\n","\n","      # patch merging layer\n","      if downsample is not None:\n","          self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n","      else:\n","          self.downsample = None\n","\n","  def forward(self, x, x_size):\n","      for blk in self.blocks:\n","          if self.use_checkpoint:\n","              x = checkpoint.checkpoint(blk, x, x_size)\n","          else:\n","              x = blk(x, x_size)\n","      if self.downsample is not None:\n","          x = self.downsample(x)\n","      return x\n","\n","  def extra_repr(self) -> str:\n","      return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n","\n","  def flops(self):\n","      flops = 0\n","      for blk in self.blocks:\n","          flops += blk.flops()\n","      if self.downsample is not None:\n","          flops += self.downsample.flops()\n","      return flops\n","\n","\n","class RSTB(nn.Module):  #RSTB\n","    \"\"\"Residual Swin Transformer Block (RSTB).\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","        img_size: Input image size.\n","        patch_size: Patch size.\n","        resi_connection: The convolutional block before residual connection.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n","                 img_size=224, patch_size=4, resi_connection='1conv'):\n","        super(RSTB, self).__init__()\n","\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","\n","        self.residual_group = BasicLayer(dim=dim,\n","                                         input_resolution=input_resolution,\n","                                         depth=depth,\n","                                         num_heads=num_heads,\n","                                         window_size=window_size,\n","                                         mlp_ratio=mlp_ratio,\n","                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                         drop=drop, attn_drop=attn_drop,\n","                                         drop_path=drop_path,\n","                                         norm_layer=norm_layer,\n","                                         downsample=downsample,\n","                                         use_checkpoint=use_checkpoint)\n","\n","        if resi_connection == '1conv':\n","            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n","                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n","\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","    def forward(self, x, x_size):\n","        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n","\n","    def flops(self):\n","        flops = 0\n","        flops += self.residual_group.flops()\n","        H, W = self.input_resolution\n","        flops += H * W * self.dim * self.dim * 9\n","        flops += self.patch_embed.flops()\n","        flops += self.patch_unembed.flops()\n","\n","        return flops\n","\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.img_size\n","        if self.norm is not None:\n","            flops += H * W * self.embed_dim\n","        return flops\n","\n","\n","class PatchUnEmbed(nn.Module):\n","    r\"\"\" Image to Patch Unembedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, x, x_size):\n","        B, HW, C = x.shape\n","        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        return flops\n","\n","\n","class Upsample(nn.Sequential):    #inside HQ Image Reconstruction\n","    \"\"\"Upsample module.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat):\n","        m = []\n","        if (scale & (scale - 1)) == 0:  # scale = 2^n\n","            for _ in range(int(math.log(scale, 2))):\n","                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n","                m.append(nn.PixelShuffle(2))\n","        elif scale == 3:\n","            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n","            m.append(nn.PixelShuffle(3))\n","        else:\n","            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n","        super(Upsample, self).__init__(*m)\n","\n","\n","class UpsampleOneStep(nn.Sequential):   #inside HQ Image reconstruction\n","    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n","       Used in lightweight SR to save parameters.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n","        self.num_feat = num_feat\n","        self.input_resolution = input_resolution\n","        m = []\n","        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n","        m.append(nn.PixelShuffle(scale))\n","        super(UpsampleOneStep, self).__init__(*m)\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.num_feat * 3 * 9\n","        return flops\n","\n","\n","class SwinIR(nn.Module):\n","    r\"\"\" SwinIR\n","        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n","\n","    Args:\n","        img_size (int | tuple(int)): Input image size. Default 64\n","        patch_size (int | tuple(int)): Patch size. Default: 1\n","        in_chans (int): Number of input image channels. Default: 3\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n","        img_range: Image range. 1. or 255.\n","        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n","        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n","    \"\"\"\n","\n","    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n","                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n","                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n","                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n","                 **kwargs):\n","        super(SwinIR, self).__init__()\n","        num_in_ch = in_chans\n","        num_out_ch = in_chans\n","        num_feat = 64\n","        self.img_range = img_range\n","        if in_chans == 3:\n","            rgb_mean = (0.4488, 0.4371, 0.4040)\n","            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n","        else:\n","            self.mean = torch.zeros(1, 1, 1, 1)\n","        self.upscale = upscale\n","        self.upsampler = upsampler\n","        self.window_size = window_size\n","\n","        #####################################################################################################\n","        ################################### 1, shallow feature extraction ###################################\n","        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n","\n","        #####################################################################################################\n","        ################################### 2, deep feature extraction ######################################\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.num_features = embed_dim\n","        self.mlp_ratio = mlp_ratio\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        num_patches = self.patch_embed.num_patches\n","        patches_resolution = self.patch_embed.patches_resolution\n","        self.patches_resolution = patches_resolution\n","\n","        # merge non-overlapping patches into image\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","\n","        # absolute position embedding\n","        if self.ape:\n","            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","            trunc_normal_(self.absolute_pos_embed, std=.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build Residual Swin Transformer blocks (RSTB)\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = RSTB(dim=embed_dim,\n","                         input_resolution=(patches_resolution[0],\n","                                           patches_resolution[1]),\n","                         depth=depths[i_layer],\n","                         num_heads=num_heads[i_layer],\n","                         window_size=window_size,\n","                         mlp_ratio=self.mlp_ratio,\n","                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                         drop=drop_rate, attn_drop=attn_drop_rate,\n","                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n","                         norm_layer=norm_layer,\n","                         downsample=None,\n","                         use_checkpoint=use_checkpoint,\n","                         img_size=img_size,\n","                         patch_size=patch_size,\n","                         resi_connection=resi_connection\n","\n","                         )\n","            self.layers.append(layer)\n","        self.norm = norm_layer(self.num_features)\n","\n","        # build the last conv layer in deep feature extraction\n","        if resi_connection == '1conv':\n","            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n","\n","        #####################################################################################################\n","        ################################ 3, high quality image reconstruction ################################\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.upsample = Upsample(upscale, num_feat)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR (to save parameters)\n","            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n","                                            (patches_resolution[0], patches_resolution[1]))\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR (less artifacts)\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            if self.upscale == 4:\n","                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'absolute_pos_embed'}\n","\n","    @torch.jit.ignore\n","    def no_weight_decay_keywords(self):\n","        return {'relative_position_bias_table'}\n","\n","    def check_image_size(self, x):\n","        _, _, h, w = x.size()\n","        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n","        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n","        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n","        return x\n","\n","    def forward_features(self, x):\n","        x_size = (x.shape[2], x.shape[3])\n","        x = self.patch_embed(x)\n","        if self.ape:\n","            x = x + self.absolute_pos_embed\n","        x = self.pos_drop(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, x_size)\n","\n","        x = self.norm(x)  # B L C\n","        x = self.patch_unembed(x, x_size)\n","\n","        return x\n","\n","    def forward(self, x):\n","        H, W = x.shape[2:]\n","        x = self.check_image_size(x)\n","        \n","        self.mean = self.mean.type_as(x)\n","        x = (x - self.mean) * self.img_range\n","\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            x = self.conv_first(x)                                              #Shallow Feature Extraction\n","            x = self.conv_after_body(self.forward_features(x)) + x              #Deep Feature Extraction + x\n","            x = self.conv_before_upsample(x)                                    #HQ Image Reconstruction\n","            x = self.conv_last(self.upsample(x))                                #HQ Image Reconstruction\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.upsample(x)\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.conv_before_upsample(x)\n","            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            if self.upscale == 4:\n","                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            x_first = self.conv_first(x)\n","            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n","            x = x + self.conv_last(res)\n","\n","        x = x / self.img_range + self.mean\n","\n","        return x[:, :, :H*self.upscale, :W*self.upscale]\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.patches_resolution\n","        flops += H * W * 3 * self.embed_dim * 9\n","        flops += self.patch_embed.flops()\n","        for i, layer in enumerate(self.layers):\n","            flops += layer.flops()\n","        flops += H * W * 3 * self.embed_dim * self.embed_dim\n","        flops += self.upsample.flops()\n","        return flops\n","\n","\n"],"metadata":{"id":"OaJdFaGlcLkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SwinIR(upscale=scale_factor, in_chans=3, img_size=CROP_SIZE, window_size=8, img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffle', resi_conv='1conv').to(device)\n","model = model.float()\n","\n","criterion = nn.MSELoss()\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","\n","hist_loss_train = []\n","hist_loss_test = []\n","hist_psnr_train = []\n","hist_psnr_test = []\n","\n","best_loss_test = float('inf')\n","best_model = model\n","\n","# resume training in case colab kernal collapse\n","# checkpoint = torch.load(\"model_SwinIR.pt\")\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","\n","for epoch in range(nb_epochs):\n","\n","    # Train\n","    avg_psnr = 0\n","    epoch_loss = 0\n","    for iteration, batch in enumerate(tqdm(trainloader)):\n","      input, target = batch[0].to(device), batch[1].to(device)\n","      input, target = input.float(), target.float()\n","      optimizer.zero_grad()\n","      out = model(input)\n","      loss = criterion(out, target)\n","      psnr = 10 * log10(1 / loss.item())\n","      loss.backward()\n","      optimizer.step()\n","      epoch_loss += loss.item()\n","      avg_psnr += psnr\n","\n","    print(f\"Epoch {epoch}. Training loss: {epoch_loss / len(trainloader)}\")\n","    hist_loss_train.append(epoch_loss / len(trainloader))\n","    hist_psnr_train.append(avg_psnr / len(trainloader))\n","\n","    # val\n","    avg_psnr = 0\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in valloader:\n","            input, target = batch[0].to(device), batch[1].to(device)\n","            input, target = input.float(), target.float()\n","            out = model(input)\n","            loss = criterion(out, target)\n","            psnr = 10 * log10(1 / loss.item())\n","            epoch_loss += loss.item()\n","            avg_psnr += psnr\n","\n","    print(f\"Average PSNR: {avg_psnr / len(valloader)} dB.\")\n","    hist_loss_test.append(epoch_loss / len(valloader))\n","    hist_psnr_test.append(avg_psnr / len(valloader))\n","\n","    if hist_loss_test[-1] < best_loss_test:\n","      best_loss_test = hist_loss_test[-1]\n","      best_model = model\n","\n","      torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, \"model_SwinIR_u\"+str(scale_factor) + \".pt\")\n","# modify and save model name as model_SwinIR_$upscaling factor$ \n","# u4 stand for upsample 4  \n","train_array = np.array(hist_loss_train)\n","val_array = np.array(hist_loss_test)\n","np.save('trainhis_u'+str(scale_factor)+'.npy',train_array)\n","np.save('valhis_u' +str(scale_factor)+'.npy',val_array)\n","model = best_model\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_K2SPu5iAlln","outputId":"af5b68e9-969e-4c1e-a1a5-14056bcee394"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","100%|██████████| 40/40 [00:26<00:00,  1.52it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 0. Training loss: 7.016593843698502\n","Average PSNR: -2.8046807957994773 dB.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1. Training loss: 1.3782151266932487\n","Average PSNR: -0.6407607109297391 dB.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [00:17<00:00,  2.24it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2. Training loss: 0.9094905659556389\n","Average PSNR: 1.0618754747987835 dB.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3. Training loss: 0.5812629751861096\n","Average PSNR: 2.93646675818851 dB.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4. Training loss: 0.4286546792834997\n","Average PSNR: 3.952678473680595 dB.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 5. Training loss: 0.3390325203537941\n","Average PSNR: 4.809982741158368 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6. Training loss: 0.2860738344490528\n","Average PSNR: 5.437813716204587 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7. Training loss: 0.24240606725215913\n","Average PSNR: 6.183234416770468 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8. Training loss: 0.2135925019159913\n","Average PSNR: 6.2741988482931195 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9. Training loss: 0.19341482650488614\n","Average PSNR: 7.157388813028892 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10. Training loss: 0.17351429145783187\n","Average PSNR: 7.515472824044575 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11. Training loss: 0.1605913070961833\n","Average PSNR: 7.6776210418553195 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12. Training loss: 0.15610567815601825\n","Average PSNR: 8.054378524548948 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13. Training loss: 0.14073181683197616\n","Average PSNR: 8.354811062840243 dB.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:17<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14. Training loss: 0.13676551757380367\n","Average PSNR: 8.348005590433846 dB.\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 12/40 [00:05<00:12,  2.25it/s]"]}]},{"cell_type":"code","source":["model = SwinIR(upscale=scale_factor, in_chans=3, img_size=CROP_SIZE, window_size=8, img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffle', resi_conv='1conv').to(device)\n","model = model.float()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","checkpoint = torch.load(\"model_SwinIR.pt\")\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","\n","model.eval()\n","avg_psnr = 0\n","epoch_loss = 0\n","with torch.no_grad():\n","    for batch in testloader:\n","        input, target = batch[0].to(device), batch[1].to(device)\n","        input, target = input.float(), target.float()\n","\n","        out = model(input)\n","        loss = criterion(out, target)\n","        psnr = 10 * log10(1 / loss.item())\n","        epoch_loss += loss.item()\n","        avg_psnr += psnr\n","\n","print(f\"Average PSNR: {avg_psnr / len(testloader)} dB.\")\n","\n","#Errors\n","MSEfunc = nn.MSELoss()\n","errors_L1 = []\n","error_L2 = 0\n","top_err = 0\n","bot_err = 0\n","testloader = DataLoader(dataset=testset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS)\n","with torch.no_grad():\n","    for idx,batch in enumerate(tqdm(testloader)):\n","        input, target = batch[0].to(device), batch[1].to(device)\n","        input, target = input.float(), target.float()\n","        out = model(input)\n","        top_err += MSEfunc(out, target).item()\n","        bot_err += MSEfunc(target,torch.zeros_like(target)).item()\n","        # even though the mathmetic representation is same for both equation -- above and sqrt(np.sum((out-target))**2/np.sum((target))**2). However, for different batch_sizes, the numpy approch just can't work\n","print('relative error: %.6f' % np.sqrt(top_err/bot_err))"],"metadata":{"id":"egEq0xpWLLJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #plot loss & psnr\n","\n","# _, ax = plt.subplots(1,2)\n","# ax[0].plot(hist_loss_train, label='train loss', c='b')\n","# ax[0].plot(hist_loss_test, label='test loss', c='r')\n","# ax[0].legend()\n","# ax[1].plot(hist_psnr_train, label='train psnr', c='b', linestyle='--')\n","# ax[1].plot(hist_psnr_test, label='test psnr', c='r', linestyle='--')\n","# ax[1].legend()\n","# _.set_size_inches(12,5)"],"metadata":{"id":"8u5jPjNVGxzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #plot example\n","# tests, targets = next(iter(testloader))\n","\n","# with torch.no_grad():\n","#   GT = targets[0].squeeze()\n","#   HR = model(tests[0][None, :].to(device).float()).squeeze()\n","\n","#   # _, ax = plt.subplots(1,2)\n","#   # ax[0].imshow(HR.detach().cpu().numpy())\n","#   # ax[1].imshow(GT)\n","#   # ax[0].title.set_text('HR SwinIR')\n","#   # ax[1].title.set_text('GT Ground Truth')\n","\n","#   # _.set_size_inches(13,13)\n","#   # plt.show()"],"metadata":{"id":"FVEYA48EMTpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zwRMSnNCMT_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from mpl_toolkits.axes_grid1 import make_axes_locatable\n","\n","# fig, ax = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(20,10))\n","# MAX = [0.1, 0.2, 0.5, 1, 10, 100]\n","\n","# k=-1\n","# im=[]\n","# for i in range(2):\n","#     for j in range(3):\n","#         k += 1\n","#         im.append(ax[i,j].imshow(errors_L1.mean(axis=0), vmax=MAX[k]))\n","#         divider = make_axes_locatable(ax[i,j])\n","#         cax = divider.append_axes('right', size='5%', pad=0.05)\n","#         fig.colorbar(im[k], cax=cax, orientation='vertical')\n","\n","# plt.suptitle('relative L1 error')"],"metadata":{"id":"1Rl-WWm8PNCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# error_L2"],"metadata":{"id":"7DjHw7SNIdPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UkhHFTFdIdyt"},"execution_count":null,"outputs":[]}]}